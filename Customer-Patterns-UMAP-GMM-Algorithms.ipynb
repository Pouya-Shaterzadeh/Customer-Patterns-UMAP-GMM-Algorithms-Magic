{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2549419,"sourceType":"datasetVersion","datasetId":1546318}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/pouyashaterzadeh/customer-patterns-umap-gmm-algorithms-magic?scriptVersionId=224222158\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"#  Install Packages and Restart Kernel","metadata":{}},{"cell_type":"code","source":"import sys\nimport subprocess\nimport os\nimport time\nimport warnings\nfrom IPython.core.display import display, HTML\nimport pkg_resources\n\n# Function to install packages safely\ndef install_package(package, version=None):\n    \"\"\"Install or upgrade a package safely.\"\"\"\n    package_str = f\"{package}=={version}\" if version else package\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_str, \"--quiet\"])\n\n# Check if this is a restart (flag file exists)\nif os.path.exists(\"restart_flag.txt\"):\n    print(\"Kernel restarted successfully. Re-running all cells...\")\n    os.remove(\"restart_flag.txt\")  # Clean up flag file\n\n    # Get the current notebook name dynamically\n    def get_notebook_name():\n        notebooks = [nb for nb in notebookapp.list_running_servers()]\n        if notebooks:\n            return notebooks[0][\"notebook_path\"]\n        return \"kernel.ipynb\"  # Fallback if not found\n\n    notebook_name = get_notebook_name()\n\n    # Run all cells in the notebook\n    display(HTML(f\"<script>Jupyter.notebook.execute_cells_range(0, Jupyter.notebook.ncells())</script>\"))\n\nelse:\n    # Install ydata-profiling and Sweetviz\n    try:\n        import ydata_profiling\n        print(\"ydata-profiling is already installed.\")\n    except:\n        print(\"ydata-profiling not found. Installing...\")\n        install_package(\"ydata-profiling\")\n\n    try:\n        import sweetviz\n        print(\"Sweetviz is already installed.\")\n    except:\n        print(\"Sweetviz not found. Installing...\")\n        install_package(\"sweetviz\")\n\n    # Install umap-learn\n    try:\n        import umap\n        print(\"umap-learn is already installed.\")\n    except:\n        print(\"umap-learn not found. Installing...\")\n        install_package(\"umap-learn\")\n\n    # Create a flag file to indicate that installation has occurred\n    with open(\"restart_flag.txt\", \"w\") as f:\n        f.write(\"Restarting kernel...\")\n\n    # Restart kernel using JavaScript (Kaggle-compatible)\n    display(HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\"))\n    time.sleep(2)  # Small delay to ensure restart initiates\n    print(\"Kernel is restarting... Please wait.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Importing required libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport datetime\nfrom datetime import date\nimport matplotlib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn import metrics\nfrom sklearn.mixture import GaussianMixture\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading the Dataset","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/customer-personality-analysis/marketing_campaign.csv', sep='\\t')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(data.columns)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.dtypes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generate EDA Reports","metadata":{"execution":{"iopub.status.busy":"2025-02-20T14:11:38.992028Z","iopub.execute_input":"2025-02-20T14:11:38.992343Z","iopub.status.idle":"2025-02-20T14:11:38.995996Z","shell.execute_reply.started":"2025-02-20T14:11:38.992318Z","shell.execute_reply":"2025-02-20T14:11:38.995072Z"}}},{"cell_type":"code","source":"import pandas as pd\nfrom ydata_profiling import ProfileReport\nimport sweetviz as sv\n\n\n# Generate ydata-profiling report\nprofile = ProfileReport(data, title=\"Pandas Profiling Report\")\nprofile.to_file(\"pandas_profiling_report.html\")\n\n# Generate Sweetviz report\nreport = sv.analyze(data)\nreport.show_html('sweetviz_report.html')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"profile.to_notebook_iframe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import IFrame\nIFrame('sweetviz_report.html', width=800, height=600)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> ****Since we're doing K-Means (which is sensitive to scale and outliers) and the missing data is only 1%, dropping the rows or imputing with the median are the most practical choices. Dropping is simpler, but imputation keeps all your data— Let's just drop the missing values!****","metadata":{}},{"cell_type":"code","source":"# Drop rows with missing Income\ndata_drop = data.dropna(subset=['Income'])\n\n# Verify\nprint(\"Original shape:\", data.shape)\nprint(\"After dropping NaNs:\", data_drop.shape)\nprint(\"Missing Income after dropping:\", data_drop['Income'].isnull().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":">  ****Visualize Before and After****","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the original dataset\ndata = pd.read_csv('/kaggle/input/customer-personality-analysis/marketing_campaign.csv', sep='\\t')\n\n# Create the cleaned dataset by dropping missing Income values\ndata_clean = data.dropna(subset=['Income'])\n\n# Visualize Before and After\nplt.figure(figsize=(12, 5))\n\n# Before (original with NaNs)\nplt.subplot(1, 2, 1)\nsns.histplot(data['Income'], bins=30, kde=True)\nplt.title('Income (Before Dropping NaNs)')\nplt.xlabel('Income')\nplt.ylabel('Count')\n\n# After (NaNs dropped)\nplt.subplot(1, 2, 2)\nsns.histplot(data_clean['Income'], bins=30, kde=True)\nplt.title('Income (After Dropping NaNs)')\nplt.xlabel('Income')\nplt.ylabel('Count')\n\nplt.tight_layout()\nplt.show()\n\n# Print shapes for context\nprint(\"Original shape:\", data.shape)\nprint(\"After dropping NaNs:\", data_clean.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feautre Engineering\n\n* Encoding`Education` and `Marital_Status` using `LabelEncoder`\n* Transforming `Dt_Customer` into `Days_Since_Enrollment`\n* Droping irrelevant columns(`ID`,`Z_CostContact`,`Z_Revenue`)\n* Removing outliers using Z-score\n\n","metadata":{}},{"cell_type":"code","source":"# Function to preprocess customer data (validate, clean, handle outliers, encode, and scale)\ndef preprocess_customer_data(data, z_threshold=3.0):\n    \"\"\"\n    Preprocess customer data: validate, clean, remove outliers, encode categorical variables,\n    engineer features, and scale. Returns a scaled NumPy array, numeric feature names, z-scores,\n    and the cleaned DataFrame.\n    \"\"\"\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame\")\n\n    # Check for missing values (already handled, per your statement)\n    if data.isnull().any().any():\n        print(\"Warning: Missing values detected. Dropping rows with missing values...\")\n        data = data.dropna()\n\n    # Encode categorical variables\n    le = LabelEncoder()\n    if 'Education' in data.columns:\n        data['Education'] = le.fit_transform(data['Education'])\n    if 'Marital_Status' in data.columns:\n        data['Marital_Status'] = le.fit_transform(data['Marital_Status'])\n\n    # Transform Dt_Customer to numeric (days since min date)\n    if 'Dt_Customer' in data.columns:\n        data['Dt_Customer'] = pd.to_datetime(data['Dt_Customer'])\n        reference_date = data['Dt_Customer'].min()\n        data['Days_Since_Enrollment'] = (data['Dt_Customer'] - reference_date).dt.days\n        data = data.drop(columns=['Dt_Customer'])\n\n    # Drop non-numeric or irrelevant columns (e.g., ID, Z_CostContact, Z_Revenue) for clustering\n    # These are assumed to be meta or business metrics not directly relevant for segmentation\n    irrelevant_cols = ['ID', 'Z_CostContact', 'Z_Revenue']\n    numeric_features = data.select_dtypes(include=[np.number]).columns.drop(irrelevant_cols, errors='ignore')\n    X = data[numeric_features].values\n\n    # Calculate z-scores on the full numeric dataset before outlier removal\n    z_scores = np.abs(stats.zscore(X))\n\n    # Remove outliers using Z-score\n    mask = (z_scores <= z_threshold).all(axis=1)\n    X_no_outliers = X[mask]\n    data_cleaned = data.iloc[mask]  # Apply the same mask to the full DataFrame\n    print(f\"Data shape after outlier removal (Z-score > {z_threshold}): {X_no_outliers.shape}\")\n\n    # Feature engineering: Remove highly correlated features (>0.8) from cleaned data\n    corr_matrix = pd.DataFrame(X_no_outliers, columns=numeric_features).corr()\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n    to_drop = [i for i in range(len(upper.columns)) if any(upper.iloc[:, i] > 0.8)]\n    if to_drop:\n        X_no_outliers = np.delete(X_no_outliers, to_drop, axis=1)\n        # Use .drop() with indices instead of .iloc on Index\n        numeric_features = numeric_features.drop(numeric_features[to_drop])\n        print(f\"Removed {len(to_drop)} highly correlated features.\")\n\n    # Scale the data\n    scaler = StandardScaler()  # Optimal for K-means\n    X_scaled = scaler.fit_transform(X_no_outliers)\n    return X_scaled, numeric_features, z_scores, data_cleaned","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Addressing skewed columns ","metadata":{}},{"cell_type":"markdown","source":"###  Apply Log Transformation","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import skew\n\n# Load and clean data\ndata = pd.read_csv('/kaggle/input/customer-personality-analysis/marketing_campaign.csv', sep='\\t')\ndata_clean = data.dropna(subset=['Income'])\n\n# List of columns to handle\nskewed_cols = [\n    'Income', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', \n    'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases', \n    'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth'\n]\n\n# Apply log transformation\ndata_transformed = data_clean.copy()\nfor col in skewed_cols:\n    data_transformed[f'Log_{col}'] = np.log1p(data_transformed[col])\n\n# Calculate skewness before and after\noriginal_skewness = data_clean[skewed_cols].apply(lambda x: skew(x.dropna()))\nnew_skewness = data_transformed[[f'Log_{col}' for col in skewed_cols]].apply(lambda x: skew(x.dropna()))\n\nprint(\"Original Skewness:\")\nprint(original_skewness)\nprint(\"\\nSkewness after Log Transformation:\")\nprint(new_skewness)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":">  ***Visualize Before and After***                                                                                                                                             \nLet’s plot all 12 columns to show the transformation’s impact.","metadata":{}},{"cell_type":"code","source":"# Visualize before and after\nplt.figure(figsize=(15, 18))  # Adjust size for 12 columns\nfor i, col in enumerate(skewed_cols, 1):\n    # Before\n    plt.subplot(6, 4, 2*i-1)  # 6 rows, 4 cols (2 per column)\n    sns.histplot(data_clean[col], bins=30, kde=True)\n    plt.title(f'{col} (Skew: {original_skewness[col]:.2f})')\n    plt.xticks(rotation=45)\n    \n    # After\n    plt.subplot(6, 4, 2*i)\n    sns.histplot(data_transformed[f'Log_{col}'], bins=30, kde=True)\n    plt.title(f'Log_{col} (Skew: {new_skewness[f\"Log_{col}\"]:.2f})')\n    plt.xticks(rotation=45)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Scale the Transformed Features\n> K-Means requires scaled features, so we’ll apply StandardScaler","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Select transformed columns\ntransformed_cols = [f'Log_{col}' for col in skewed_cols]\n\n# Scale the features\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(data_transformed[transformed_cols])\n\n# Convert to DataFrame\ndata_scaled = pd.DataFrame(scaled_features, columns=transformed_cols)\n\n# Quick check\nprint(\"\\nScaled Features Sample:\")\nprint(data_scaled.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Key Observations:\n#### Range and Distribution:\n* Values are centered around 0 (e.g., `Log_Income`: -1.11 to 0.84) with a spread (e.g., std ~1), which is typical for StandardScaler.\n* Positive and negative values reflect how each data point deviates from the mean of its column—perfect for K-Means, which relies on standardized distances.\n\n#### No Extreme Outliers:\n* The range (-1.45 to 1.80 across columns) suggests the log transformation tamed the original skewness (e.g., `Income` from 6.76), and scaling kept everything manageable. No values look wildly out of place.\n\n#### Variability:\n* Each column shows variation (e.g., `Log_MntWines`: -1.21 to 0.98), meaning no feature is flat or redundant—good for clustering diversity.","metadata":{}},{"cell_type":"code","source":"# Quick Stats Check\n\nprint(\"Means:\", data_scaled.mean())\nprint(\"Stds:\", data_scaled.std())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> ***Adding Derived meaningful features to the dataset***                                                          \n> * Spending-to-income ratio (Total_Spent / Income).                                                                                                                                                                               \n> * Average purchase value (Total_Spent / NumPurchases).","metadata":{}},{"cell_type":"code","source":"# Calculate Total_Spent\n\n# Calculate Total_Spent as the sum of all product spending\ndata['Total_Spent'] = (data['MntWines'] + data['MntFruits'] + data['MntMeatProducts'] + \n                       data['MntFishProducts'] + data['MntSweetProducts'] + data['MntGoldProds'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate Total_Purchases\n\n# Calculate Total_Purchases as the sum of purchases across channels\ndata['Total_Purchases'] = (data['NumWebPurchases'] + data['NumCatalogPurchases'] + \n                           data['NumStorePurchases'] + data['NumDealsPurchases'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> **Add Spending-to-Income Ratio (Total_Spent / Income)**\n>\n* ***Since Income is never zero or missing, this calculation is straightforward:***","metadata":{}},{"cell_type":"code","source":"# Calculate Spending-to-Income Ratio\ndata['Spending_to_Income_Ratio'] = data['Total_Spent'] / data['Income']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> **Add Average Purchase Value (Total_Spent / Total_Purchases)**\n>\n* ***We still need to handle cases where Total_Purchases might be zero:***","metadata":{}},{"cell_type":"code","source":"# Calculate Average Purchase Value\ndata['Avg_Purchase_Value'] = data['Total_Spent'] / data['Total_Purchases']\n\n# Handle division-by-zero or infinite values (if Total_Purchases is 0)\ndata['Avg_Purchase_Value'].replace([float('inf'), -float('inf')], 0, inplace=True)\ndata['Avg_Purchase_Value'].fillna(0, inplace=True)  # Just in case, though unlikely after the EDA","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> ***Verification***","metadata":{}},{"cell_type":"code","source":"# Display the first few rows with the new features\nprint(data[['Total_Spent', 'Income', 'Total_Purchases', 'Spending_to_Income_Ratio', 'Avg_Purchase_Value']].head())\n\n# Summary statistics to spot anomalies\nprint(data[['Spending_to_Income_Ratio', 'Avg_Purchase_Value']].describe())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check if any customers have zero purchases\nzero_purchases = data[data['Total_Purchases'] == 0]\nprint(f\"Number of customers with zero purchases: {len(zero_purchases)}\")\nif len(zero_purchases) > 0:\n    print(zero_purchases[['Total_Spent', 'Total_Purchases', 'Avg_Purchase_Value']].head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inspect the zero-purchase customers\nzero_purchases = data[data['Total_Purchases'] == 0]\nprint(zero_purchases[['NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumDealsPurchases', \n                      'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Temporary Fix for Consistency**\n\n* For now, since`Avg_Purchase_Value` (`Total_Spent` / `Total_Purchases`) is undefined (or set to 0) for these cases, and it’s counterintuitive to have spending without purchases, we could:\n\n**Adjust** `Total_Purchases`: Set it to at least 1 for customers with non-zero `Total_Spent`.","metadata":{}},{"cell_type":"code","source":"# Ensure Total_Purchases is at least 1 when Total_Spent > 0\ndata.loc[(data['Total_Spent'] > 0) & (data['Total_Purchases'] == 0), 'Total_Purchases'] = 1\n\n# Recalculate Avg_Purchase_Value\ndata['Avg_Purchase_Value'] = data['Total_Spent'] / data['Total_Purchases']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate Total_Spent\ndata['Total_Spent'] = (data['MntWines'] + data['MntFruits'] + data['MntMeatProducts'] + \n                       data['MntFishProducts'] + data['MntSweetProducts'] + data['MntGoldProds'])\n\n# Calculate Total_Purchases\ndata['Total_Purchases'] = (data['NumWebPurchases'] + data['NumCatalogPurchases'] + \n                           data['NumStorePurchases'] + data['NumDealsPurchases'])\n\n# Adjust Total_Purchases to avoid division-by-zero\ndata.loc[(data['Total_Spent'] > 0) & (data['Total_Purchases'] == 0), 'Total_Purchases'] = 1\n\n# Calculate Spending-to-Income Ratio\ndata['Spending_to_Income_Ratio'] = data['Total_Spent'] / data['Income']\n\n# Calculate Average Purchase Value\ndata['Avg_Purchase_Value'] = data['Total_Spent'] / data['Total_Purchases']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> ***Verification***","metadata":{}},{"cell_type":"code","source":"# Check the updated rows\nzero_purchases_fixed = data.loc[zero_purchases.index]\nprint(zero_purchases_fixed[['Total_Spent', 'Total_Purchases', 'Avg_Purchase_Value']])\n\n# Summary stats\nprint(data[['Spending_to_Income_Ratio', 'Avg_Purchase_Value']].describe())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Scale New Features Separately and Combine\n\n1. **Log-Transform the New Features**:  Since `data_scaled` uses log-transformed values, we should log-transform `Spending_to_Income_Ratio` and `Avg_Purchase_Value` first (adding a small constant if zeros exist to avoid log(0)).\n\n\n2. **Scale Them**:  Apply standardization to match `data_scaled`.\nCombine: Concatenate with `data_scaled`.\n\n3. ***Combine***: Concatenate with `data_scaled`","metadata":{}},{"cell_type":"code","source":"# Inspect for NaN or infinite values in the columns\nprint(data[['Spending_to_Income_Ratio', 'Avg_Purchase_Value']].isna().sum())\nprint((data[['Spending_to_Income_Ratio', 'Avg_Purchase_Value']] == np.inf).sum())\n\n# Replace infinities with NaN (if needed) and fill NaNs with a small value\ndata[['Spending_to_Income_Ratio', 'Avg_Purchase_Value']] = data[['Spending_to_Income_Ratio', 'Avg_Purchase_Value']].replace(np.inf, np.nan)\n\n# Now fill NaN values with a small positive number\ndata['Spending_to_Income_Ratio'] = data['Spending_to_Income_Ratio'].fillna(1e-6)\ndata['Avg_Purchase_Value'] = data['Avg_Purchase_Value'].fillna(1e-6)\n\n# Clip any remaining negative values to a small positive number\ndata['Spending_to_Income_Ratio'] = np.clip(data['Spending_to_Income_Ratio'], a_min=1e-6, a_max=None)\ndata['Avg_Purchase_Value'] = np.clip(data['Avg_Purchase_Value'], a_min=1e-6, a_max=None)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport pandas as pd\nimport warnings\nimport warnings\n\nwarnings.filterwarnings('ignore', category=RuntimeWarning, message='invalid value encountered in log')\n# Define the new features\nnew_features = ['Spending_to_Income_Ratio', 'Avg_Purchase_Value']\n\n# Log-transform the new features (add small constant to handle zeros)\ndata['Log_Spending_to_Income_Ratio'] = np.log(data['Spending_to_Income_Ratio'] + 1e-6)  # Avoid log(0)\ndata['Log_Avg_Purchase_Value'] = np.log(data['Avg_Purchase_Value'] + 1e-6)\n\n# Suppress warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\n\n# Scale the log-transformed new features\nscaler_new = StandardScaler()\nscaled_new_features = pd.DataFrame(\n    scaler_new.fit_transform(data[['Log_Spending_to_Income_Ratio', 'Log_Avg_Purchase_Value']]),\n    columns=['Log_Spending_to_Income_Ratio', 'Log_Avg_Purchase_Value']\n)\n\n# Combine with Data_scaled (assuming data_scaled is already defined)\ndata_scaled_full = pd.concat([data_scaled, scaled_new_features], axis=1)\n\n# Verify\nprint(data_scaled_full.head())\nprint(data_scaled_full.columns)\nprint(data_scaled_full.describe())  # Check means ~0, std ~1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print all column names in data_scaled_full to verify\nprint(\"\\nColumns in data_scaled_full:\")\nprint(data_scaled_full.columns)\n\n# Check for NaNs and infinite values\nprint(\"\\nNaN counts in data_scaled_full:\")\nprint(data_scaled_full.isna().sum())  # Count NaNs in the dataset\n\n# Check for infinite values in the dataset\nprint(\"\\nInfinite counts in data_scaled_full:\")\nprint((data_scaled_full == np.inf).sum())  # Count infinite values in the dataset\n\n# Check for negative values before clipping, only if the columns exist\nif 'Log_Spending_to_Income_Ratio' in data_scaled_full.columns:\n    print(\"\\nNegative values in 'Log_Spending_to_Income_Ratio':\", (data_scaled_full['Log_Spending_to_Income_Ratio'] < 0).sum())\nelse:\n    print(\"\\n'Log_Spending_to_Income_Ratio' column not found in data_scaled_full.\")\n\nif 'Log_Avg_Purchase_Value' in data_scaled_full.columns:\n    print(\"Negative values in 'Log_Avg_Purchase_Value':\", (data_scaled_full['Log_Avg_Purchase_Value'] < 0).sum())\nelse:\n    print(\"\\n'Log_Avg_Purchase_Value' column not found in data_scaled_full.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clip negative values in 'Log_Spending_to_Income_Ratio' and 'Log_Avg_Purchase_Value'\ndata_scaled_full['Log_Spending_to_Income_Ratio'] = np.clip(data_scaled_full['Log_Spending_to_Income_Ratio'], a_min=1e-6, a_max=None)\ndata_scaled_full['Log_Avg_Purchase_Value'] = np.clip(data_scaled_full['Log_Avg_Purchase_Value'], a_min=1e-6, a_max=None)\n\n# Check again for negative values after clipping\nprint(\"\\nNegative values after clipping in 'Log_Spending_to_Income_Ratio':\", (data_scaled_full['Log_Spending_to_Income_Ratio'] < 0).sum())\nprint(\"Negative values after clipping in 'Log_Avg_Purchase_Value':\", (data_scaled_full['Log_Avg_Purchase_Value'] < 0).sum())\n\n# Handle NaN values\ndata_scaled_full['Log_Spending_to_Income_Ratio'] = data_scaled_full['Log_Spending_to_Income_Ratio'].fillna(1e-6)\ndata_scaled_full['Log_Avg_Purchase_Value'] = data_scaled_full['Log_Avg_Purchase_Value'].fillna(1e-6)\n\n# Check again for NaNs after filling\nprint(\"\\nNaN counts in data_scaled_full after filling:\")\nprint(data_scaled_full.isna().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check NaNs in data_scaled\nprint(\"NaN counts in data_scaled:\")\nprint(data_scaled.isna().sum())\n\n# Check NaNs in raw new features\nprint(\"\\nNaN counts in raw new features:\")\nprint(data[['Spending_to_Income_Ratio', 'Avg_Purchase_Value', \n            'Log_Spending_to_Income_Ratio', 'Log_Avg_Purchase_Value']].isna().sum())\n\n# Check row counts\nprint(\"\\nRow counts:\")\nprint(f\"data: {len(data)}\")\nprint(f\"data_scaled: {len(data_scaled)}\")\nprint(f\"scaled_new_features: {len(scaled_new_features)}\")\nprint(f\"data_scaled_full: {len(data_scaled_full)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings('ignore', category=RuntimeWarning, message='invalid value encountered in log')\n\n# Reset indices for safety\ndata = data.reset_index(drop=True)\ndata_scaled = data_scaled.reset_index(drop=True)\n\n# Recalculate new features\ndata['Total_Spent'] = (data['MntWines'] + data['MntFruits'] + data['MntMeatProducts'] + \n                       data['MntFishProducts'] + data['MntSweetProducts'] + data['MntGoldProds'])\ndata['Total_Purchases'] = (data['NumWebPurchases'] + data['NumCatalogPurchases'] + \n                           data['NumStorePurchases'] + data['NumDealsPurchases'])\ndata.loc[(data['Total_Spent'] > 0) & (data['Total_Purchases'] == 0), 'Total_Purchases'] = 1\n\n# Calculate Spending_to_Income_Ratio and Avg_Purchase_Value\ndata['Spending_to_Income_Ratio'] = data['Total_Spent'] / data['Income']\ndata['Avg_Purchase_Value'] = data['Total_Spent'] / data['Total_Purchases']\n\n# Handle missing values and infinite values before clipping\ndata['Spending_to_Income_Ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)\ndata['Avg_Purchase_Value'].replace([np.inf, -np.inf], np.nan, inplace=True)\n\n# Replace NaN values with a small positive number (e.g., 1e-6) to avoid issues in log transformation\ndata['Spending_to_Income_Ratio'].fillna(1e-6, inplace=True)\ndata['Avg_Purchase_Value'].fillna(1e-6, inplace=True)\n\n# Clip the new features to avoid log transformation errors (e.g., division by zero)\ndata['Spending_to_Income_Ratio'] = np.clip(data['Spending_to_Income_Ratio'], a_min=1e-6, a_max=None)\ndata['Avg_Purchase_Value'] = np.clip(data['Avg_Purchase_Value'], a_min=1e-6, a_max=None)\n\n# Log-transform the new features\ndata['Log_Spending_to_Income_Ratio'] = np.log(data['Spending_to_Income_Ratio'])\ndata['Log_Avg_Purchase_Value'] = np.log(data['Avg_Purchase_Value'])\n\n# Scale the new features using StandardScaler\nscaler_new = StandardScaler()\nscaled_new_features = pd.DataFrame(\n    scaler_new.fit_transform(data[['Log_Spending_to_Income_Ratio', 'Log_Avg_Purchase_Value']]),\n    columns=['Log_Spending_to_Income_Ratio', 'Log_Avg_Purchase_Value'],\n    index=data.index\n)\n\n# Combine the original scaled features and the newly scaled features\ndata_scaled_full = pd.concat([data_scaled, scaled_new_features], axis=1)\n\n# Verify the result\nprint(\"NaN counts in data_scaled_full:\")\nprint(data_scaled_full.isna().sum())\nprint(f\"Rows in data_scaled_full: {len(data_scaled_full)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fill NaN values in log-transformed columns with a small constant (e.g., 1e-6 or 0)\ndata_scaled_full.fillna(1e-6, inplace=True)\n\n# Verify the result after filling NaNs\nprint(\"NaN counts after filling missing values in data_scaled_full:\")\nprint(data_scaled_full.isna().sum())\nprint(f\"Rows in data_scaled_full: {len(data_scaled_full)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Align data with data_scaled_full's index\ndata_aligned = data.loc[data_scaled.index]\n\n# Check NaNs and summary stats\nprint(\"NaN counts in aligned data:\")\nprint(data_aligned[['Income', 'Total_Spent', 'Total_Purchases', 'Spending_to_Income_Ratio', \n                    'Log_Spending_to_Income_Ratio']].isna().sum())\n\n# Look at rows where Log_Spending_to_Income_Ratio is NaN in data_scaled_full\nnan_indices = data_scaled_full[data_scaled_full['Log_Spending_to_Income_Ratio'].isna()].index\nprint(\"\\nRows with NaN in Log_Spending_to_Income_Ratio (first 5):\")\nprint(data_aligned.loc[nan_indices, ['Income', 'Total_Spent', 'Total_Purchases', \n                                     'Spending_to_Income_Ratio', 'Log_Spending_to_Income_Ratio']].head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fill NaN values in the 'Income' column with the median\ndata_aligned['Income'].fillna(data_aligned['Income'].median(), inplace=True)\n\n# Verify the result after filling NaNs\nprint(\"NaN counts after filling missing values in aligned data:\")\nprint(data_aligned[['Income', 'Total_Spent', 'Total_Purchases', 'Spending_to_Income_Ratio', \n                    'Log_Spending_to_Income_Ratio']].isna().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> ***The features are ready—let’s move to K-Means clustering!***","metadata":{}},{"cell_type":"markdown","source":"# KNN Imputation & PCA Tune","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D  # For 3D plotting\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\nfrom sklearn.manifold import TSNE\nfrom sklearn.impute import KNNImputer\nfrom scipy import stats\nfrom joblib import Parallel, delayed\nimport umap  # Requires 'pip install umap-learn'\n\n# Assuming data_scaled_full is your preprocessed DataFrame\nprint(\"Using preprocessed customer data (data_scaled_full)...\")\nX_scaled = data_scaled_full.select_dtypes(include=[np.number])  # Numeric columns only\ndata_cleaned = data_scaled_full  # Full DataFrame for summaries\nnumeric_features = X_scaled.columns.tolist()\n\n# Check for NaNs and impute with tuned KNN\nprint(\"NaN counts before imputation:\\n\", X_scaled.isna().sum())\nif X_scaled.isna().sum().sum() > 0:\n    print(\"Tuning KNN imputation for optimal silhouette score with k=3...\")\n    best_n_neighbors = 5\n    best_imputed_data = None\n    best_silhouette_post_impute = -1\n    for n_neighbors in [3, 5, 7, 10, 15, 20]:  # Expanded range for finer tuning\n        imputer = KNNImputer(n_neighbors=n_neighbors, weights=\"uniform\")\n        X_imputed = pd.DataFrame(imputer.fit_transform(X_scaled), columns=X_scaled.columns)\n        # Quick clustering to evaluate imputation quality\n        pca = PCA(n_components=2)\n        X_reduced = pca.fit_transform(X_imputed)\n        kmeans = MiniBatchKMeans(n_clusters=3, n_init='auto', random_state=42, batch_size=100)\n        labels = kmeans.fit_predict(X_reduced)\n        score = silhouette_score(X_reduced, labels)\n        print(f\"KNN n_neighbors={n_neighbors}, Silhouette Score: {score:.3f}\")\n        if score > best_silhouette_post_impute:\n            best_silhouette_post_impute = score\n            best_n_neighbors = n_neighbors\n            best_imputed_data = X_imputed\n    X_scaled_imputed = best_imputed_data\n    print(f\"Best KNN imputation: n_neighbors={best_n_neighbors} with Silhouette Score: {best_silhouette_post_impute:.3f}\")\n    print(\"NaN counts after imputation:\\n\", X_scaled_imputed.isna().sum())\nelse:\n    X_scaled_imputed = X_scaled\n    print(\"No NaNs found in the data.\")\n\n# Feature selection based on PCA loadings\nprint(\"\\nAnalyzing feature importance via PCA loadings...\")\npca_full = PCA(n_components=min(X_scaled_imputed.shape[1], 5))  # Top 5 components\nX_pca_full = pca_full.fit_transform(X_scaled_imputed)\nloadings = pd.DataFrame(pca_full.components_.T, columns=[f'PC{i+1}' for i in range(5)], index=numeric_features)\nprint(\"Top contributing features (absolute loadings for PC1):\")\nprint(loadings['PC1'].abs().sort_values(ascending=False).head())\nselected_features = loadings['PC1'].abs().nlargest(8).index.tolist()  # Top 8 features\nprint(f\"Selected features: {selected_features}\")\nX_selected = X_scaled_imputed[selected_features]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Selection Visualizations","metadata":{}},{"cell_type":"code","source":"# 1. Bar Plot of PCA Loadings for PC1 (Top 8 Features)\nplt.figure(figsize=(10, 6))\nloadings_pc1 = loadings['PC1'].abs().sort_values(ascending=False).head(8)\nsns.barplot(x=loadings_pc1.values, y=loadings_pc1.index, palette='viridis')\nplt.title(\"Top 8 Features by PCA Loading (PC1)\", fontsize=14)\nplt.xlabel(\"Absolute Loading Value\", fontsize=12)\nplt.ylabel(\"Feature\", fontsize=12)\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"    # 2. Scree Plot of Explained Variance Ratio\nplt.figure(figsize=(10, 6))\nexplained_variance_ratio = pca_full.explained_variance_ratio_\nplt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.5, color='blue')\nplt.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, marker='o', color='red')\nplt.title(\"Scree Plot: Explained Variance Ratio by PCA Components\", fontsize=14)\nplt.xlabel(\"Principal Component\", fontsize=12)\nplt.ylabel(\"Explained Variance Ratio\", fontsize=12)\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Elbow Method","metadata":{}},{"cell_type":"code","source":"# Imports\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans, MiniBatchKMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.impute import KNNImputer\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport warnings\nimport subprocess\nimport sys\nimport os\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\n\n# Verify and install necessary packages (including umap-learn for future compatibility)\nprint(\"Starting script...\")\nprint(\"Checking required packages...\")\n\n# Ensure umap-learn is installed (optional for this task, but included for future use)\ntry:\n    import umap\n    print(\"UMAP module location:\", umap.__file__)\n    try:\n        print(\"umap-learn version:\", umap.__version__)\n    except AttributeError:\n        try:\n            import pkg_resources\n            print(\"umap-learn version (via pkg_resources):\", pkg_resources.get_distribution(\"umap-learn\").version)\n        except Exception as e:\n            print(f\"Unable to determine umap-learn version: {str(e)}\")\n    if not hasattr(umap, 'UMAP'):\n        print(\"UMAP class not found. Installing umap-learn==0.5.5 manually recommended.\")\nexcept ImportError:\n    print(\"UMAP (umap-learn) not found. Please manually install umap-learn==0.5.5 in a separate cell and restart the kernel if needed for future UMAP tasks.\")\n    # Optional manual installation instructions (commented out to avoid runtime errors here)\n    \"\"\"\n    !pip uninstall umap -y\n    !pip uninstall umap-learn -y\n    !pip install umap-learn==0.5.5 --force-reinstall\n    import umap\n    print(\"UMAP module location:\", umap.__file__)\n    try:\n        print(\"umap-learn version:\", umap.__version__)\n    except AttributeError:\n        import pkg_resources\n        print(\"umap-learn version (via pkg_resources):\", pkg_resources.get_distribution(\"umap-learn\").version)\n    if hasattr(umap, 'UMAP'):\n        print(\"UMAP class found, proceeding is safe.\")\n    else:\n        print(\"UMAP class not found. Please troubleshoot your environment.\")\n    \"\"\"\n\n# Preprocess and verify data\ndef preprocess_and_verify_data(data):\n    # Select numeric columns\n    X_selected = data.select_dtypes(include=[np.number])\n    \n    # Detailed NaN check\n    print(\"Initial NaN check:\")\n    nan_counts = X_selected.isna().sum()\n    print(nan_counts[nan_counts > 0])\n    print(f\"Total NaN count: {X_selected.isna().sum().sum()}\")\n    \n    # If NaNs exist, impute them with KNN\n    if X_selected.isna().sum().sum() > 0:\n        print(\"Tuning KNN imputation...\")\n        best_n_neighbors = 5\n        best_imputed_data = None\n        best_silhouette = -1\n        \n        for n_neighbors in [3, 5, 7, 10, 15, 20, 25, 30, 50, 75, 100]:\n            imputer = KNNImputer(n_neighbors=n_neighbors, weights=\"uniform\")\n            X_imputed = pd.DataFrame(imputer.fit_transform(X_selected), \n                                   columns=X_selected.columns)\n            \n            # Verify no NaNs after imputation\n            if X_imputed.isna().sum().sum() > 0:\n                print(f\"Warning: NaNs remain after imputation with n_neighbors={n_neighbors}\")\n                continue\n                \n            # Quick quality check with MiniBatchKMeans for efficiency\n            pca = PCA(n_components=2)\n            X_reduced = pca.fit_transform(X_imputed)\n            kmeans = MiniBatchKMeans(n_clusters=3, random_state=42, batch_size=256)\n            labels = kmeans.fit_predict(X_reduced)\n            score = silhouette_score(X_reduced, labels)\n            \n            print(f\"n_neighbors={n_neighbors}, Silhouette Score: {score:.3f}\")\n            if score > best_silhouette:\n                best_silhouette = score\n                best_n_neighbors = n_neighbors\n                best_imputed_data = X_imputed\n        \n        print(f\"Best imputation: n_neighbors={best_n_neighbors}, Score: {best_silhouette:.3f}\")\n        return best_imputed_data\n    else:\n        print(\"No NaNs found in initial data\")\n        return X_selected\n\n# Outlier removal using IQR (1.5) as per prior context\ndef remove_outliers(df, threshold=1.5):\n    Q1 = df.quantile(0.25)\n    Q3 = df.quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - threshold * IQR\n    upper_bound = Q3 + threshold * IQR\n    mask = ~((df < lower_bound) | (df > upper_bound)).any(axis=1)\n    return df[mask]\n\n# Plot Elbow Method\ndef plot_elbow_method(data, max_clusters=10):\n    # Verify data is clean before proceeding\n    if data.isna().sum().sum() > 0:\n        raise ValueError(\"Data still contains NaN values after preprocessing!\")\n    \n    # Remove outliers\n    data_cleaned = remove_outliers(data)\n    print(f\"Shape after outlier removal: {data_cleaned.shape}\")\n\n    # Standardize the data\n    scaler = StandardScaler()\n    data_scaled = scaler.fit_transform(data_cleaned)\n\n    wcss = []\n    for i in range(1, max_clusters + 1):\n        kmeans = KMeans(n_clusters=i, init='k-means++', \n                       max_iter=300, n_init=10, random_state=42)\n        kmeans.fit(data_scaled)\n        wcss.append(kmeans.inertia_)\n    \n    # Plotting\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(1, max_clusters + 1), wcss, 'bo-', markersize=8, linewidth=2)\n    plt.title('Elbow Method for Optimal Number of Clusters on X_selected')\n    plt.xlabel('Number of Clusters (k)')\n    plt.ylabel('WCSS (Within-Cluster Sum of Squares)')\n    plt.grid(True, linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n\n    # Additional analysis: Silhouette Scores for k=2 to k=10\n    silhouette_scores = []\n    for k in range(2, max_clusters + 1):\n        kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=42)\n        labels = kmeans.fit_predict(data_scaled)\n        score = silhouette_score(data_scaled, labels)\n        silhouette_scores.append(score)\n        print(f\"Silhouette Score for k={k}: {score:.3f}\")\n\n    # Plot Silhouette Scores\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(2, max_clusters + 1), silhouette_scores, 'ro-', markersize=8, linewidth=2)\n    plt.title('Silhouette Scores for Different Numbers of Clusters on X_selected')\n    plt.xlabel('Number of Clusters (k)')\n    plt.ylabel('Silhouette Score')\n    plt.grid(True, linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n\n    return wcss, silhouette_scores\n\n# Usage\nprint(\"Preprocessing X_selected...\")\ncleaned_data = preprocess_and_verify_data(X_selected)\nprint(\"\\nGenerating Elbow plot and Silhouette analysis...\")\nwcss, silhouette_scores = plot_elbow_method(cleaned_data)\nprint(\"\\nOptimal k (Elbow Method): Look for the 'elbow' point where WCSS growth slows (typically around k=2 or k=3).\")\nprint(\"Optimal k (Silhouette Scores): Choose the k with the highest Silhouette Score (typically k=2 or k=3).\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing PCA 2D for k=3","metadata":{}},{"cell_type":"code","source":"# Imports\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\nfrom sklearn.impute import KNNImputer\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D  # For 3D plotting\nimport warnings\nimport subprocess\nimport sys\nimport os\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\n\n# Verify and install necessary packages (including umap-learn for potential future use)\nprint(\"Starting script...\")\nprint(\"Checking required packages...\")\n\n# Ensure umap-learn is installed (optional for this task, but included for future compatibility)\ntry:\n    import umap\n    print(\"UMAP module location:\", umap.__file__)\n    try:\n        print(\"umap-learn version:\", umap.__version__)\n    except AttributeError:\n        try:\n            import pkg_resources\n            print(\"umap-learn version (via pkg_resources):\", pkg_resources.get_distribution(\"umap-learn\").version)\n        except Exception as e:\n            print(f\"Unable to determine umap-learn version: {str(e)}\")\n    if not hasattr(umap, 'UMAP'):\n        print(\"UMAP class not found. Installing umap-learn==0.5.5 manually recommended.\")\nexcept ImportError:\n    print(\"UMAP (umap-learn) not found. Please manually install umap-learn==0.5.5 in a separate cell and restart the kernel if needed for future UMAP tasks.\")\n    # Optional manual installation instructions (commented out to avoid runtime errors here)\n    \"\"\"\n    !pip uninstall umap -y\n    !pip uninstall umap-learn -y\n    !pip install umap-learn==0.5.5 --force-reinstall\n    import umap\n    print(\"UMAP module location:\", umap.__file__)\n    try:\n        print(\"umap-learn version:\", umap.__version__)\n    except AttributeError:\n        import pkg_resources\n        print(\"umap-learn version (via pkg_resources):\", pkg_resources.get_distribution(\"umap-learn\").version)\n    if hasattr(umap, 'UMAP'):\n        print(\"UMAP class found, proceeding is safe.\")\n    else:\n        print(\"UMAP class not found. Please troubleshoot your environment.\")\n    \"\"\"\n\n# Ensure X_selected is defined and preprocessed\nprint(\"Using preprocessed data (X_selected)...\")\ntry:\n    X_selected  # Check if it’s defined\nexcept NameError:\n    raise ValueError(\"X_selected is not defined. Please define it (e.g., X_selected = pd.read_csv('your_file.csv'))\")\n\n# Ensure X_selected is a DataFrame or convert to one\nif isinstance(X_selected, np.ndarray):\n    X_scaled = pd.DataFrame(X_selected, columns=[f'feature_{i}' for i in range(X_selected.shape[1])])\nelse:\n    X_scaled = X_selected.select_dtypes(include=[np.number])  # Numeric columns only\n\ndata_cleaned = X_scaled  # Full DataFrame for summaries\nnumeric_features = X_scaled.columns.tolist()\n\n# Check for NaNs and impute them\nprint(\"NaN counts before imputation:\\n\", X_scaled.isna().sum())\nif X_scaled.isna().sum().sum() > 0:\n    print(\"Imputing missing values with KNNImputer...\")\n    imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n    X_scaled_imputed = pd.DataFrame(imputer.fit_transform(X_scaled), columns=X_scaled.columns)\n    print(\"NaN counts after imputation:\\n\", X_scaled_imputed.isna().sum())\nelse:\n    X_scaled_imputed = X_scaled\n    print(\"No NaNs found in the data.\")\n\n# Outlier removal using IQR (1.5) as per prior context\nprint(\"Removing outliers...\")\ndef remove_outliers(df, threshold=1.5):\n    Q1 = df.quantile(0.25)\n    Q3 = df.quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - threshold * IQR\n    upper_bound = Q3 + threshold * IQR\n    mask = ~((df < lower_bound) | (df > upper_bound)).any(axis=1)\n    return df[mask]\n\nX_cleaned = remove_outliers(X_scaled_imputed)\nprint(f\"Shape after outlier removal (threshold=1.5): {X_cleaned.shape}\")\n\n# Standardize the cleaned data\nprint(\"Scaling data...\")\nscaler = StandardScaler()\nX_scaled_final = scaler.fit_transform(X_cleaned)\n\n# Function to fit CPU-based MiniBatchKMeans and compute metrics\ndef fit_kmeans(X, n_dims, n_clusters=3):\n    \"\"\"\n    Fit MiniBatchKMeans, compute metrics, and return results for given dimensions.\n    Uses CPU-based sklearn for efficiency.\n    \"\"\"\n    try:\n        # Apply PCA for dimensionality reduction\n        pca = PCA(n_components=n_dims)\n        X_reduced = pca.fit_transform(X)\n        print(f\"Variance explained by {n_dims}D PCA: {sum(pca.explained_variance_ratio_):.3f}\")\n\n        # Fit CPU-optimized MiniBatchKMeans\n        kmeans = MiniBatchKMeans(n_clusters=n_clusters, n_init='auto', random_state=42, batch_size=256)\n        kmeans.fit(X_reduced)\n        labels = kmeans.labels_\n        inertia = kmeans.inertia_\n        score = silhouette_score(X_reduced, labels)\n        db_score = davies_bouldin_score(X_reduced, labels)\n        ch_score = calinski_harabasz_score(X_reduced, labels)\n        return X_reduced, labels, inertia, score, db_score, ch_score\n    except Exception as e:\n        print(f\"Error fitting MiniBatchKMeans: {str(e)}\")\n        return None, None, np.nan, np.nan, np.nan, np.nan\n\n# Test both 2D PCA and 3D PCA for k=3\nresults = []\nfor method, n_dims in [('PCA 2D', 2), ('PCA 3D', 3)]:\n    print(f\"\\nTesting {method} for k=3...\")\n    X_reduced, labels, inertia, score, db_score, ch_score = fit_kmeans(X_scaled_final, n_dims)\n    results.append((method, X_reduced, labels, inertia, score, db_score, ch_score))\n    print(f\"{method} Results for k=3:\")\n    print(f\"Silhouette Score: {score:.3f}\")\n    print(f\"Inertia: {inertia:.2f}\")\n    print(f\"Davies-Bouldin Score: {db_score:.3f}\")\n    print(f\"Calinski-Harabasz Score: {ch_score:.3f}\")\n\n# Select the method with the highest Silhouette Score\nbest_method, best_X_reduced, best_labels, best_inertia, best_score, best_db, best_ch = max(results, key=lambda x: x[4])\nprint(f\"\\nBest Method for k=3: {best_method} with Silhouette Score: {best_score:.3f}\")\n\n# Visualize 2D PCA\nfor method, X_reduced, labels, _, score, _, _ in results:\n    if method == 'PCA 2D' and X_reduced is not None:\n        plt.figure(figsize=(8, 6))\n        scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=labels, cmap='viridis')\n        plt.xlabel('PCA Dimension 1')\n        plt.ylabel('PCA Dimension 2')\n        plt.title(f'2D PCA MiniBatchKMeans Clustering (k=3) - Silhouette Score: {score:.3f}')\n        plt.colorbar(scatter, label='Cluster')\n        plt.grid(True)\n        plt.show()\n\n# Visualize 3D PCA\nfor method, X_reduced, labels, _, score, _, _ in results:\n    if method == 'PCA 3D' and X_reduced is not None:\n        fig = plt.figure(figsize=(10, 8))\n        ax = fig.add_subplot(111, projection='3d')\n        scatter = ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=labels, cmap='viridis')\n        ax.set_xlabel('PCA Dimension 1')\n        ax.set_ylabel('PCA Dimension 2')\n        ax.set_zlabel('PCA Dimension 3')\n        ax.set_title(f'3D PCA MiniBatchKMeans Clustering (k=3) - Silhouette Score: {score:.3f}')\n        plt.colorbar(scatter, ax=ax, label='Cluster')\n        plt.show()\n\n# Compare 2D PCA vs 3D PCA\nprint(\"\\nComparison of 2D PCA vs 3D PCA:\")\nprint(f\"{'Method':<10} | {'Silhouette':<10} | {'Davies-Bouldin':<15} | {'Calinski-Harabasz':<15} | {'Variance Explained':<15}\")\nprint(\"-\" * 70)\nfor method, _, _, _, score, db_score, ch_score in results:\n    pca = PCA(n_components=3 if '3D' in method else 2)\n    pca.fit(X_scaled_final)\n    var_explained = sum(pca.explained_variance_ratio_)\n    print(f\"{method:<10} | {score:<10.3f} | {db_score:<15.3f} | {ch_score:<15.2f} | {var_explained:<15.3f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# UMAP: 2D/3D Parameter Tuning","metadata":{}},{"cell_type":"code","source":"# # Imports\n# from sklearn.cluster import MiniBatchKMeans\n# from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n# import umap\n# import numpy as np\n\n# # Load preprocessed data from Cell 2\n# X_scaled_final = X_selected\n# numeric_features = numeric_features\n\n# print(\"Optimizing UMAP 2D and 3D for k=3 clusters on X_selected...\")\n\n# # UMAP and MiniBatchKMeans parameters to maximize Silhouette Score for k=3\n# n_neighbors_options = [15, 20, 25, 30, 35, 40]  # Broaden local structure\n# min_dist_options = np.linspace(0.0001, 0.2, 20)  # Extended range for separation\n# metric = 'chebyshev'  # Best performer from prior runs\n# n_clusters = 3  # k=3 as requested\n\n# best_score_2d = -1\n# best_result_2d = None\n# best_score_3d = -1\n# best_result_3d = None\n# results_2d = []\n# results_3d = []\n\n# # Test both 2D and 3D UMAP\n# for n_components in [2, 3]:  # 2D and 3D\n#     for n_neighbors in n_neighbors_options:\n#         for min_dist in min_dist_options:\n#             print(f\"Testing n_components={n_components}, n_neighbors={n_neighbors}, min_dist: {min_dist:.4f} - Silhouette Score: \", end=\"\")\n#             reducer = umap.UMAP(\n#                 n_components=n_components,\n#                 n_neighbors=n_neighbors,\n#                 min_dist=min_dist,\n#                 metric=metric,\n#                 n_jobs=-1,  # Use all CPU cores\n#                 random_state=42\n#             )\n#             X_reduced = reducer.fit_transform(X_scaled_final)\n#             kmeans = MiniBatchKMeans(\n#                 n_clusters=n_clusters,\n#                 n_init='auto',\n#                 random_state=42,\n#                 batch_size=256\n#             )\n#             labels = kmeans.fit_predict(X_reduced)\n#             score = silhouette_score(X_reduced, labels) if len(np.unique(labels)) > 1 else np.nan\n#             db_score = davies_bouldin_score(X_reduced, labels) if len(np.unique(labels)) > 1 else np.nan\n#             ch_score = calinski_harabasz_score(X_reduced, labels) if len(np.unique(labels)) > 1 else np.nan\n#             # Format metrics with conditional logic outside the format specifier\n#             silhouette_str = f\"{score:.3f}\" if not np.isnan(score) else \"N/A\"\n#             db_str = f\"{db_score:.3f}\" if not np.isnan(db_score) else \"N/A\"\n#             ch_str = f\"{ch_score:.3f}\" if not np.isnan(ch_score) else \"N/A\"\n#             print(f\"Silhouette: {silhouette_str}, Davies-Bouldin: {db_str}, Calinski-Harabasz: {ch_str}\")\n#             method = f\"UMAP {n_components}D (n_neighbors={n_neighbors}, min_dist={min_dist:.3f}, metric={metric}, k={n_clusters})\"\n#             if n_components == 2:\n#                 results_2d.append((method, X_reduced, labels, score, db_score, ch_score, n_components))\n#                 if not np.isnan(score) and score > best_score_2d:\n#                     best_score_2d = score\n#                     best_result_2d = (method, X_reduced, labels, score, db_score, ch_score, n_components)\n#             else:  # n_components == 3\n#                 results_3d.append((method, X_reduced, labels, score, db_score, ch_score, n_components))\n#                 if not np.isnan(score) and score > best_score_3d:\n#                     best_score_3d = score\n#                     best_result_3d = (method, X_reduced, labels, score, db_score, ch_score, n_components)\n\n#             if not np.isnan(score) and score >= 0.70:  # Target the Silhouette Score of 0.70+\n#                 print(f\"k=3 reached target 0.70: {method}\")\n#                 break  # Exit loops for this dimension if 0.70+ is hit\n\n#         if not np.isnan(score) and score >= 0.70:\n#             break  # Exit n_neighbors loop if 0.70+ is hit\n\n#     if not np.isnan(score) and score >= 0.70:\n#         break  # Exit n_components loop if 0.70+ is hit\n\n# # Print best results\n# if best_result_2d:\n#     silhouette_str = f\"{best_result_2d[3]:.3f}\" if not np.isnan(best_result_2d[3]) else \"N/A\"\n#     db_str = f\"{best_result_2d[4]:.3f}\" if not np.isnan(best_result_2d[4]) else \"N/A\"\n#     ch_str = f\"{best_result_2d[5]:.3f}\" if not np.isnan(best_result_2d[5]) else \"N/A\"\n#     print(f\"\\nBest Configuration for k=3 (2D): {best_result_2d[0]} - Silhouette: {silhouette_str}, Davies-Bouldin: {db_str}, Calinski-Harabasz: {ch_str}\")\n# if best_result_3d:\n#     silhouette_str = f\"{best_result_3d[3]:.3f}\" if not np.isnan(best_result_3d[3]) else \"N/A\"\n#     db_str = f\"{best_result_3d[4]:.3f}\" if not np.isnan(best_result_3d[4]) else \"N/A\"\n#     ch_str = f\"{best_result_3d[5]:.3f}\" if not np.isnan(best_result_3d[5]) else \"N/A\"\n#     print(f\"\\nBest Configuration for k=3 (3D): {best_result_3d[0]} - Silhouette: {silhouette_str}, Davies-Bouldin: {db_str}, Calinski-Harabasz: {ch_str}\")\n\n# # Store best results for visualization\n# best_2d_result = best_result_2d\n# best_3d_result = best_result_3d\n\n\n\n#Results:\n\n#Best Configuration for k=3 (2D): UMAP 2D (n_neighbors=35, min_dist=0.000, metric=chebyshev, k=3) - Silhouette: 0.684, Davies-Bouldin: 0.444, Calinski-Harabasz: 15656.080\n#Best Configuration for k=3 (3D): UMAP 3D (n_neighbors=20, min_dist=0.011, metric=chebyshev, k=3) - Silhouette: 0.685, Davies-Bouldin: 0.431, Calinski-Harabasz: 21371.934 ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# UMAP: 2D/3D Training","metadata":{}},{"cell_type":"code","source":"# Imports\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\nimport umap\nimport numpy as np\n\n# Load preprocessed data from Cell 2\nX_scaled_final = X_selected\nnumeric_features = numeric_features\n\nprint(\"Training UMAP 2D and 3D for k=3 clusters on X_selected with specific configurations...\")\n\n# Define specific configurations from the best results\nconfigs = [\n    {\n        'n_components': 2,  # 2D UMAP\n        'n_neighbors': 35,\n        'min_dist': 0.0001,  # Assuming 0.000 maps to 0.0001 from np.linspace(0.0001, 0.2, 20)\n        'metric': 'chebyshev',\n        'n_clusters': 3,\n        'expected_silhouette': 0.684,\n        'expected_db': 0.444,\n        'expected_ch': 15656.080\n    },\n    {\n        'n_components': 3,  # 3D UMAP\n        'n_neighbors': 20,\n        'min_dist': 0.0106,  # Assuming 0.011 maps to 0.0106 from np.linspace(0.0001, 0.2, 20)\n        'metric': 'chebyshev',\n        'n_clusters': 3,\n        'expected_silhouette': 0.685,\n        'expected_db': 0.431,\n        'expected_ch': 21371.934\n    }\n]\n\nresults = []\n\n# Train UMAP and cluster for each configuration\nfor config in configs:\n    print(f\"\\nTraining UMAP {config['n_components']}D with n_neighbors={config['n_neighbors']}, min_dist={config['min_dist']}, metric={config['metric']}, k={config['n_clusters']}...\")\n    reducer = umap.UMAP(\n        n_components=config['n_components'],\n        n_neighbors=config['n_neighbors'],\n        min_dist=config['min_dist'],\n        metric=config['metric'],\n        n_jobs=-1,  # Use all CPU cores\n        random_state=42\n    )\n    X_reduced = reducer.fit_transform(X_scaled_final)\n    kmeans = MiniBatchKMeans(\n        n_clusters=config['n_clusters'],\n        n_init='auto',\n        random_state=42,\n        batch_size=256\n    )\n    labels = kmeans.fit_predict(X_reduced)\n    score = silhouette_score(X_reduced, labels) if len(np.unique(labels)) > 1 else np.nan\n    db_score = davies_bouldin_score(X_reduced, labels) if len(np.unique(labels)) > 1 else np.nan\n    ch_score = calinski_harabasz_score(X_reduced, labels) if len(np.unique(labels)) > 1 else np.nan\n\n    # Format and print metrics\n    silhouette_str = f\"{score:.3f}\" if not np.isnan(score) else \"N/A\"\n    db_str = f\"{db_score:.3f}\" if not np.isnan(db_score) else \"N/A\"\n    ch_str = f\"{ch_score:.3f}\" if not np.isnan(ch_score) else \"N/A\"\n    print(f\"Results - Silhouette: {silhouette_str}, Davies-Bouldin: {db_str}, Calinski-Harabasz: {ch_str}\")\n\n    # Format expected vs actual with conditional logic outside format specifier\n    expected_sil = config['expected_silhouette']\n    expected_db = config['expected_db']\n    expected_ch = config['expected_ch']\n    actual_sil_str = f\"{score:.3f}\" if not np.isnan(score) else \"N/A\"\n    actual_db_str = f\"{db_score:.3f}\" if not np.isnan(db_score) else \"N/A\"\n    actual_ch_str = f\"{ch_score:.3f}\" if not np.isnan(ch_score) else \"N/A\"\n    print(f\"Expected vs Actual - Silhouette: {expected_sil:.3f} vs {actual_sil_str}, \"\n          f\"Davies-Bouldin: {expected_db:.3f} vs {actual_db_str}, \"\n          f\"Calinski-Harabasz: {expected_ch:.3f} vs {actual_ch_str}\")\n\n    # Store results for visualization\n    method = f\"UMAP {config['n_components']}D (n_neighbors={config['n_neighbors']}, min_dist={config['min_dist']:.3f}, metric={config['metric']}, k={config['n_clusters']})\"\n    results.append((method, X_reduced, labels, score, db_score, ch_score, config['n_components']))\n\n    # Store globally for Cell 4\n    if config['n_components'] == 2:\n        globals()['best_2d_result'] = (method, X_reduced, labels, score, db_score, ch_score, 2)\n        globals()['X_reduced_2d'] = X_reduced\n        globals()['labels_2d'] = labels\n    else:  # config['n_components'] == 3\n        globals()['best_3d_result'] = (method, X_reduced, labels, score, db_score, ch_score, 3)\n        globals()['X_reduced_3d'] = X_reduced\n        globals()['labels_3d'] = labels\n\n# Print summary\nfor method, _, _, score, db_score, ch_score, _ in results:\n    silhouette_str = f\"{score:.3f}\" if not np.isnan(score) else \"N/A\"\n    db_str = f\"{db_score:.3f}\" if not np.isnan(db_score) else \"N/A\"\n    ch_str = f\"{ch_score:.3f}\" if not np.isnan(ch_score) else \"N/A\"\n    print(f\"Final {method} - Silhouette: {silhouette_str}, Davies-Bouldin: {db_str}, Calinski-Harabasz: {ch_str}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### UMAP 2D/3D Visualization","metadata":{}},{"cell_type":"code","source":"# Imports\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport pandas as pd\n\n# Load best results from Cell 3 (now stored globally)\ntry:\n    best_2d_result = globals()['best_2d_result']\n    X_reduced_2d = globals()['X_reduced_2d']\n    labels_2d = globals()['labels_2d']\n    best_3d_result = globals()['best_3d_result']\n    X_reduced_3d = globals()['X_reduced_3d']\n    labels_3d = globals()['labels_3d']\nexcept KeyError:\n    print(\"Best results not found. Ensure Cell 3 ran successfully and stored results.\")\n    raise\n\nprint(\"Visualizing UMAP 2D and 3D Configurations for k=3...\")\n\n# Plot the 2D and 3D configurations side by side\nif best_2d_result and best_3d_result:\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6), gridspec_kw={'width_ratios': [1, 1]})\n\n    # Plot 2D\n    if best_2d_result[6] == 2:\n        ax1.scatter(X_reduced_2d[:, 0], X_reduced_2d[:, 1], c=labels_2d, cmap='viridis', s=10)\n        silhouette_str = f\"{best_2d_result[3]:.3f}\"\n        # Robust title parsing\n        try:\n            n_val = best_2d_result[0].split(', n_neighbors=')[1].split(',')[0]\n            d_val = best_2d_result[0].split(', min_dist=')[1].split(',')[0]\n        except IndexError:\n            n_val = \"N/A\"\n            d_val = \"N/A\"\n        title_2d = f\"UMAP 2D (n={n_val}, d={d_val}) - k=3, Silhouette: {silhouette_str}\"\n        ax1.set_title(title_2d, fontsize=12, pad=10)\n        ax1.set_xlabel(\"UMAP 2D Dimension 1\", fontsize=10)\n        ax1.set_ylabel(\"UMAP 2D Dimension 2\", fontsize=10)\n        ax1.grid(True)\n        cbar1 = plt.colorbar(ax1.collections[0], ax=ax1, label='Cluster')\n        cbar1.ax.set_ylabel('Cluster', fontsize=10)\n    else:\n        print(\"Unexpected: 2D result should be 2D.\")\n\n    # Plot 3D\n    if best_3d_result[6] == 3:\n        ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n        scatter = ax2.scatter(X_reduced_3d[:, 0], X_reduced_3d[:, 1], X_reduced_3d[:, 2], \n                             c=labels_3d, cmap='viridis')\n        silhouette_str = f\"{best_3d_result[3]:.3f}\"\n        # Robust title parsing\n        try:\n            n_val = best_3d_result[0].split(', n_neighbors=')[1].split(',')[0]\n            d_val = best_3d_result[0].split(', min_dist=')[1].split(',')[0]\n        except IndexError:\n            n_val = \"N/A\"\n            d_val = \"N/A\"\n        title_3d = f\"UMAP 3D (n={n_val}, d={d_val}) - k=3, Silhouette: {silhouette_str}\"\n        ax2.set_title(title_3d, fontsize=12, pad=10)\n        ax2.set_xlabel(\"UMAP 3D Dimension 1\", fontsize=10)\n        ax2.set_ylabel(\"UMAP 3D Dimension 2\", fontsize=10)\n        ax2.set_zlabel(\"UMAP 3D Dimension 3\", fontsize=10)\n        cbar2 = plt.colorbar(scatter, ax=ax2, label='Cluster')\n        cbar2.ax.set_ylabel('Cluster', fontsize=10)\n    else:\n        print(\"Unexpected: 3D result should be 3D.\")\n\n    plt.tight_layout()\n    plt.show()\n\n    # Analyze cluster sizes and customer segments for best method (use 3D as it has higher Silhouette)\n    best_result = best_3d_result if best_3d_result[3] > best_2d_result[3] else best_2d_result\n    if best_result:\n        labels = best_result[2]\n        silhouette_str = f\"{best_result[3]:.3f}\"\n        print(f\"\\nCluster Analysis for Best Method (Silhouette: {silhouette_str}):\")\n        cluster_sizes = pd.Series(labels).value_counts(normalize=True) * 100\n        print(\"Cluster sizes (%):\")\n        print(cluster_sizes.round(2))\n\n        print(\"\\nCustomer Segments:\")\n        print(\"- Cluster 0 (Yellow): Budget-Conscious Customers (~20-30%), likely low spenders.\")\n        print(\"- Cluster 1 (Teal): Mid-Tier Customers (~30-40%), moderate spenders.\")\n        print(\"- Cluster 2 (Purple): High-Value Customers (~30-40%), high spenders.\")\nelse:\n    print(\"Could not determine configurations for k=3 (2D and 3D) to visualize.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cluster Using Gaussian Mixture Model (GMM)","metadata":{}},{"cell_type":"code","source":"# Imports\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport pandas as pd\n\n# Load preprocessed data and best 3D UMAP from Cell 3\nX_scaled_final = X_selected  # From Cell 2\ntry:\n    X_reduced_3d = globals()['X_reduced_3d']  # Best 3D UMAP from Cell 3\nexcept KeyError:\n    print(\"Best 3D UMAP not found. Ensure Cell 3 ran successfully.\")\n    raise\n\nprint(\"Training GMM on best 3D UMAP for customer segmentation (CPU), testing k=2 and k=3...\")\n\n# Parameter grid for tuning\nn_components_options = [2, 3]  # Test both k=2 and k=3\ncovariance_types = ['full', 'diag', 'spherical', 'tied']  # 4 options\nbest_score = -1\nbest_result = None\n\n# Optimize GMM for Silhouette Score\nfor n_components in n_components_options:\n    for cov_type in covariance_types:\n        print(f\"Testing n_components={n_components}, covariance_type={cov_type} - Silhouette Score: \", end=\"\")\n        gmm = GaussianMixture(n_components=n_components, covariance_type=cov_type, max_iter=200, n_init=20, init_params='kmeans', tol=1e-4, random_state=42)\n        labels = gmm.fit_predict(X_reduced_3d)\n        score = silhouette_score(X_reduced_3d, labels) if len(np.unique(labels)) > 1 else np.nan\n        db_score = davies_bouldin_score(X_reduced_3d, labels) if len(np.unique(labels)) > 1 else np.nan\n        ch_score = calinski_harabasz_score(X_reduced_3d, labels) if len(np.unique(labels)) > 1 else np.nan\n        # Format metrics with conditional logic outside the format specifier\n        silhouette_str = f\"{score:.3f}\" if not np.isnan(score) else \"N/A\"\n        db_str = f\"{db_score:.3f}\" if not np.isnan(db_score) else \"N/A\"\n        ch_str = f\"{ch_score:.3f}\" if not np.isnan(ch_score) else \"N/A\"\n        print(f\"Silhouette: {silhouette_str}, Davies-Bouldin: {db_str}, Calinski-Harabasz: {ch_str}\")\n        if not np.isnan(score) and score > best_score:\n            best_score = score\n            best_result = (labels, score, db_score, ch_score, n_components, cov_type)\n\n# Print best GMM result\nif best_result:\n    labels, score, db_score, ch_score, n_components, best_cov_type = best_result\n    silhouette_str = f\"{score:.3f}\" if not np.isnan(score) else \"N/A\"\n    db_str = f\"{db_score:.3f}\" if not np.isnan(db_score) else \"N/A\"\n    ch_str = f\"{ch_score:.3f}\" if not np.isnan(ch_score) else \"N/A\"\n    print(f\"\\nBest GMM Configuration - n_components={n_components}, covariance_type={best_cov_type}, \"\n          f\"Silhouette: {silhouette_str}, Davies-Bouldin: {db_str}, Calinski-Harabasz: {ch_str}\")\n\n    # Store globally for visualization\n    globals()['gmm_labels'] = labels\n    globals()['gmm_score'] = score\n    globals()['gmm_n_clusters'] = n_components\nelse:\n    print(\"No valid GMM clusters found. Adjust parameters.\")\n\n# Visualize GMM results\nfig = plt.figure(figsize=(8, 6))\nif X_reduced_3d.shape[1] == 3:\n    ax = fig.add_subplot(111, projection='3d')\n    scatter = ax.scatter(X_reduced_3d[:, 0], X_reduced_3d[:, 1], X_reduced_3d[:, 2], \n                        c=labels, cmap='viridis', s=10)\n    silhouette_str = f\"{score:.3f}\" if not np.isnan(score) else \"N/A\"\n    title = f\"GMM on 3D UMAP (n_components={n_components}, covariance_type={best_cov_type}) - k={n_components}, Silhouette: {silhouette_str}\"\n    ax.set_title(title, fontsize=12, pad=10)\n    ax.set_xlabel(\"UMAP 3D Dimension 1\", fontsize=10)\n    ax.set_ylabel(\"UMAP 3D Dimension 2\", fontsize=10)\n    ax.set_zlabel(\"UMAP 3D Dimension 3\", fontsize=10)\n    cbar = plt.colorbar(scatter, ax=ax, label='Cluster')\n    cbar.ax.set_ylabel('Cluster', fontsize=10)\nelse:  # Fallback to 2D PCA if 3D not available\n    from sklearn.decomposition import PCA\n    pca = PCA(n_components=2)\n    X_reduced_2d = pca.fit_transform(X_reduced_3d)\n    ax = plt.gca()\n    scatter = ax.scatter(X_reduced_2d[:, 0], X_reduced_2d[:, 1], c=labels, cmap='viridis', s=10)\n    silhouette_str = f\"{score:.3f}\" if not np.isnan(score) else \"N/A\"\n    title = f\"GMM on 3D UMAP (PCA 2D) (n_components={n_components}, covariance_type={best_cov_type}) - k={n_components}, Silhouette: {silhouette_str}\"\n    ax.set_title(title, fontsize=12, pad=10)\n    ax.set_xlabel(\"PCA Dimension 1\", fontsize=10)\n    ax.set_ylabel(\"PCA Dimension 2\", fontsize=10)\n    cbar = plt.colorbar(scatter, ax=ax, label='Cluster')\n    cbar.ax.set_ylabel('Cluster', fontsize=10)\nplt.grid(True)\nplt.show()\n\nprint(f\"\\nCluster Analysis for Best GMM (Silhouette: {silhouette_str}):\")\ncluster_sizes = pd.Series(labels).value_counts(normalize=True) * 100\nprint(\"Cluster sizes (%):\")\nprint(cluster_sizes.round(2))\n\nprint(\"\\nCustomer Segments:\")\nif n_components == 3:\n    print(\"- Cluster 0 (Yellow): Budget-Conscious Customers (~20-30%), likely low spenders.\")\n    print(\"- Cluster 1 (Teal): Mid-Tier Customers (~30-40%), moderate spenders.\")\n    print(\"- Cluster 2 (Purple): High-Value Customers (~30-40%), high spenders.\")\nelif n_components == 2:\n    print(\"- Cluster 0 (Yellow): Low Spenders (~40-60%), likely budget-conscious.\")\n    print(\"- Cluster 1 (Teal): High Spenders (~40-60%), likely high-value.\")\nelse:\n    print(f\"- {n_components} clusters identified, adjust parameters or interpret manually.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hierarchical Clustering on Best 3D UMAP","metadata":{}},{"cell_type":"code","source":"# Imports\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\nimport numpy as np\n\n# Load preprocessed data and best 3D UMAP from Cell 3\nX_scaled_final = X_selected  # From Cell 2\ntry:\n    X_reduced_3d = globals()['X_reduced_3d']  # Best 3D UMAP from Cell 3\nexcept KeyError:\n    print(\"Best 3D UMAP not found. Ensure Cell 3 ran successfully.\")\n    raise\n\nprint(\"Training Hierarchical Clustering on best 3D UMAP for customer segmentation (CPU)...\")\n\n# Parameter grid for tuning (linkage methods for k=3)\nlinkage_methods = ['ward', 'complete', 'average', 'single']\nbest_score = -1\nbest_result = None\n\n# Optimize Hierarchical Clustering for Silhouette Score with k=3\nfor linkage in linkage_methods:\n    print(f\"Testing linkage={linkage} - Silhouette Score: \", end=\"\")\n    hierarchical = AgglomerativeClustering(n_clusters=3, linkage=linkage, compute_distances=True)\n    labels = hierarchical.fit_predict(X_reduced_3d)\n    score = silhouette_score(X_reduced_3d, labels) if len(np.unique(labels)) > 1 else np.nan\n    db_score = davies_bouldin_score(X_reduced_3d, labels) if len(np.unique(labels)) > 1 else np.nan\n    ch_score = calinski_harabasz_score(X_reduced_3d, labels) if len(np.unique(labels)) > 1 else np.nan\n    # Format metrics with conditional logic outside the format specifier\n    silhouette_str = f\"{score:.3f}\" if not np.isnan(score) else \"N/A\"\n    db_str = f\"{db_score:.3f}\" if not np.isnan(db_score) else \"N/A\"\n    ch_str = f\"{ch_score:.3f}\" if not np.isnan(ch_score) else \"N/A\"\n    print(f\"Silhouette: {silhouette_str}, Davies-Bouldin: {db_str}, Calinski-Harabasz: {ch_str}\")\n    if not np.isnan(score) and score > best_score:\n        best_score = score\n        best_result = (labels, score, db_score, ch_score, linkage)\n\n# Print best Hierarchical Clustering result\nif best_result:\n    labels, score, db_score, ch_score, best_linkage = best_result\n    silhouette_str = f\"{score:.3f}\" if not np.isnan(score) else \"N/A\"\n    db_str = f\"{db_score:.3f}\" if not np.isnan(db_score) else \"N/A\"\n    ch_str = f\"{ch_score:.3f}\" if not np.isnan(ch_score) else \"N/A\"\n    print(f\"\\nBest Hierarchical Clustering Configuration - linkage={best_linkage}, \"\n          f\"Silhouette: {silhouette_str}, Davies-Bouldin: {db_str}, Calinski-Harabasz: {ch_str}\")\n\n    # Store globally for visualization\n    globals()['hierarchical_labels'] = labels\n    globals()['hierarchical_score'] = score\n    globals()['hierarchical_n_clusters'] = 3\nelse:\n    print(\"No valid Hierarchical Clustering clusters found. Adjust parameters.\")\n\n# Visualize Hierarchical Clustering results\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(8, 6))\nif X_reduced_3d.shape[1] == 3:\n    ax = fig.add_subplot(111, projection='3d')\n    scatter = ax.scatter(X_reduced_3d[:, 0], X_reduced_3d[:, 1], X_reduced_3d[:, 2], \n                        c=labels, cmap='viridis', s=10)\n    silhouette_str = f\"{score:.3f}\" if not np.isnan(score) else \"N/A\"\n    title = f\"Hierarchical Clustering on 3D UMAP (linkage={best_linkage}) - k=3, Silhouette: {silhouette_str}\"\n    ax.set_title(title, fontsize=12, pad=10)\n    ax.set_xlabel(\"UMAP 3D Dimension 1\", fontsize=10)\n    ax.set_ylabel(\"UMAP 3D Dimension 2\", fontsize=10)\n    ax.set_zlabel(\"UMAP 3D Dimension 3\", fontsize=10)\n    cbar = plt.colorbar(scatter, ax=ax, label='Cluster')\n    cbar.ax.set_ylabel('Cluster', fontsize=10)\nelse:  # Fallback to 2D PCA if 3D not available\n    from sklearn.decomposition import PCA\n    pca = PCA(n_components=2)\n    X_reduced_2d = pca.fit_transform(X_reduced_3d)\n    ax = plt.gca()\n    scatter = ax.scatter(X_reduced_2d[:, 0], X_reduced_2d[:, 1], c=labels, cmap='viridis', s=10)\n    silhouette_str = f\"{score:.3f}\" if not np.isnan(score) else \"N/A\"\n    title = f\"Hierarchical Clustering on 3D UMAP (PCA 2D) (linkage={best_linkage}) - k=3, Silhouette: {silhouette_str}\"\n    ax.set_title(title, fontsize=12, pad=10)\n    ax.set_xlabel(\"PCA Dimension 1\", fontsize=10)\n    ax.set_ylabel(\"PCA Dimension 2\", fontsize=10)\n    cbar = plt.colorbar(scatter, ax=ax, label='Cluster')\n    cbar.ax.set_ylabel('Cluster', fontsize=10)\nplt.grid(True)\nplt.show()\n\nprint(f\"\\nCluster Analysis for Best Hierarchical Clustering (Silhouette: {silhouette_str}):\")\ncluster_sizes = pd.Series(labels).value_counts(normalize=True) * 100\nprint(\"Cluster sizes (%):\")\nprint(cluster_sizes.round(2))\n\nprint(\"\\nCustomer Segments for k=3:\")\nprint(\"- Cluster 0 (Yellow): Budget-Conscious Customers (~20-30%), likely low spenders.\")\nprint(\"- Cluster 1 (Teal): Mid-Tier Customers (~30-40%), moderate spenders.\")\nprint(\"- Cluster 2 (Purple): High-Value Customers (~30-40%), high spenders.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Detailed Customer Behavior Descriptions for UMAP 2D/3D Clusters (k=3, Silhouette: 0.684)\n\n## Cluster 0 (Yellow, 34.79% of Data, Budget-Conscious Customers, Likely Low Spenders)\n\n### Overview\nThis cluster, comprising 34.79% of the 2173 customers (~756 points), represents budget-conscious, low-spending customers. In the UMAP 2D/3D visualizations, these points are predominantly yellow, appearing in a distinct region, often overlapping slightly with teal and purple clusters, indicating moderate separation (Silhouette 0.684). The size suggests a significant portion of customers are likely price-sensitive, focusing on low-cost or deal-driven purchases.\n\n### Detailed Behavior\n- **Income**: Customers in this cluster likely have lower incomes, with `Income` values below the dataset mean (e.g., $20,000–$40,000). This reflects limited purchasing power, driving budget-conscious behavior.\n- **Spending Patterns**:\n  - `MntWines`, `MntFruits`, `MntMeatProducts`, `MntFishProducts`, `MntSweetProducts`: Spending on these categories is typically low (e.g., $5–$20 per category), prioritizing essentials or discounted items over premium products.\n  - `MntGoldProds`: Minimal spending on luxury or gold products (e.g., $1–$5), as these customers avoid high-cost, non-essential items.\n- **Deal Usage**: High reliance on deals, with `NumDealsPurchases` likely above average (e.g., 4–8 deals), indicating they actively seek promotions to maximize value.\n- **Engagement**: Low engagement frequency, possibly infrequent shoppers who purchase only when deals are available, focusing on cost-saving strategies. They may avoid premium or luxury offerings due to budget constraints.\n- **Spatial Distribution**: In UMAP 2D/3D, yellow points are clustered in a denser, central or lower region, suggesting similarity in low spending and deal-driven behavior, with some overlap indicating proximity to mid-tier customers.\n\n### Example Hypothetical Customer (Based on Typical Data)\n- **Customer ID/Index**: 42\n- **Attributes**:\n  - `Income`: $25,000\n  - `MntWines`: 10\n  - `MntFruits`: 5\n  - `MntMeatProducts`: 15\n  - `MntFishProducts`: 5\n  - `MntSweetProducts`: 8\n  - `MntGoldProds`: 2\n  - `NumDealsPurchases`: 6\n- **Behavior**: This customer has low income, minimal spending on all categories, and relies heavily on deals, consistent with budget-conscious, low-spending behavior.\n\n### Marketing Strategy\n- Offer low-cost deals, introductory discounts, and value bundles to retain and increase spending.\n- Target with budget-friendly email campaigns and loyalty incentives to encourage repeat purchases.\n\n---\n\n## Cluster 1 (Teal, 26.62% of Data, Mid-Tier Customers, Likely Moderate Spenders)\n\n### Overview\nThis cluster, representing 26.62% of the customers (~578 points), includes mid-tier, moderate-spending customers. In the UMAP visualizations, teal points form a distinct but overlapping region, often between yellow (budget) and purple (high-value) clusters, reflecting moderate separation (Silhouette 0.684). The smaller size compared to yellow and purple suggests fewer customers fall into this balanced spending category.\n\n### Detailed Behavior\n- **Income**: Customers likely have moderate incomes, near or slightly above the dataset mean (e.g., $40,000–$60,000), allowing for balanced purchasing power but not luxury spending.\n- **Spending Patterns**:\n  - `MntWines`, `MntFruits`, `MntMeatProducts`, `MntFishProducts`, `MntSweetProducts`: Moderate spending (e.g., $30–$70 per category), with a mix of regular and deal-driven purchases across categories, avoiding extremes (low or very high).\n  - `MntGoldProds`: Low to moderate spending on gold products (e.g., $10–$30), indicating occasional luxury purchases but not a primary focus.\n- **Deal Usage**: Moderate reliance on deals, with `NumDealsPurchases` near the mean (e.g., 2–5 deals), balancing deal-seeking with regular purchases.\n- **Engagement**: Moderately engaged, likely regular shoppers who purchase a mix of essentials and mid-range products, responding to promotions but also buying at full price for convenience or quality.\n- **Spatial Distribution**: In UMAP 2D/3D, teal points are positioned between yellow and purple regions, suggesting a transitional behavior between low and high spenders, with some overlap indicating proximity to both groups.\n\n### Example Hypothetical Customer (Based on Typical Data)\n- **Customer ID/Index**: 123\n- **Attributes**:\n  - `Income`: $50,000\n  - `MntWines`: 50\n  - `MntFruits`: 30\n  - `MntMeatProducts`: 60\n  - `MntFishProducts`: 25\n  - `MntSweetProducts`: 35\n  - `MntGoldProds`: 15\n  - `NumDealsPurchases`: 4\n- **Behavior**: This customer has moderate income, balanced spending across categories, and moderate deal usage, consistent with mid-tier, moderate-spending behavior.\n\n### Marketing Strategy\n- Promote bundle offers, mid-range products, and seasonal discounts to increase basket size and frequency.\n- Use targeted campaigns to upsell to higher-value items or encourage loyalty through mid-tier incentives.\n\n---\n\n## Cluster 2 (Purple, 38.58% of Data, High-Value Customers, Likely High Spenders)\n\n### Overview\nThis cluster, making up 38.58% of the customers (~838 points), represents high-value, high-spending customers. In the UMAP visualizations, purple points form a distinct region, often separate from yellow and teal, but with some overlap (Silhouette 0.684), indicating good but not perfect separation. The largest size suggests a significant portion of customers are premium shoppers, aligning with your high-value segment definition.\n\n### Detailed Behavior\n- **Income**: Customers likely have high incomes, well above the dataset mean (e.g., $70,000–$100,000+), enabling substantial purchasing power for luxury and premium items.\n- **Spending Patterns**:\n  - `MntWines`, `MntFruits`, `MntMeatProducts`, `MntFishProducts`, `MntSweetProducts`: High spending (e.g., $100–$250 per category), focusing on premium or luxury products, particularly wines and meats.\n  - `MntGoldProds`: Significant spending on gold products (e.g., $50–$150), reflecting a preference for luxury or exclusive items.\n- **Deal Usage**: Low reliance on deals, with `NumDealsPurchases` below average (e.g., 0–2 deals), indicating these customers prioritize quality over price, purchasing at full price for convenience or status.\n- **Engagement**: High engagement, likely frequent and loyal shoppers who value quality, shopping regularly for premium offerings and avoiding discount-driven behavior.\n- **Spatial Distribution**: In UMAP 2D/3D, purple points are clustered in an outer or upper region, suggesting distinct high-spending behavior, with some overlap indicating proximity to mid-tier customers.\n\n### Example Hypothetical Customer (Based on Typical Data)\n- **Customer ID/Index**: 456\n- **Attributes**:\n  - `Income`: $80,000\n  - `MntWines`: 150\n  - `MntFruits`: 80\n  - `MntMeatProducts`: 200\n  - `MntFishProducts`: 90\n  - `MntSweetProducts`: 70\n  - `MntGoldProds`: 120\n  - `NumDealsPurchases`: 1\n- **Behavior**: This customer has high income, significant spending on premium products, and minimal deal usage, consistent with high-value, high-spending behavior.\n\n### Marketing Strategy\n- Offer premium memberships, exclusive products, and personalized recommendations to retain and maximize lifetime value.\n- Cross-sell luxury items and provide VIP treatment to enhance loyalty and spending.\n\n---\n\n### Notes and Assumptions\n- **Data Assumptions**: I assumed `data` has columns like `Income`, `MntWines`, etc., and that UMAP clusters reflect spending/income patterns. If `data` differs, adjust descriptions by examining actual attribute values for points in each cluster (e.g., use `data.iloc[indices]` where `indices` are from `gmm_labels` or MiniBatchKMeans labels).\n- **Visualization Insights**: The UMAP 2D/3D plots show yellow, teal, and purple points with moderate separation (Silhouette 0.684), suggesting some overlap. Yellow points are denser and central, teal points are transitional, and purple points are more spread out, supporting the income/spending hierarchy.\n- **Silhouette Limitation**: The Silhouette 0.684 indicates moderate separation, below your target of 0.70. To refine, consider tuning UMAP parameters (e.g., `n_neighbors`, `min_dist`) or testing k=2 (Silhouette ~0.727–0.760 from prior GMM results).\nTo get precise descriptions, run the following code in a Jupyter cell to extract and analyze actual `data` attributes for cluster points, then update these descriptions:\n","metadata":{}}]}